{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:55:37.776176Z",
     "start_time": "2025-02-19T01:55:37.771157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocabulary: str, special_tokens: list):\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocabulary_dictionary = {}\n",
    "        self.reverse_vocabulary_dictionary = {}\n",
    "        vocabulary_list = list(set(vocabulary))\n",
    "        vocabulary_list = vocabulary_list + special_tokens\n",
    "        for index, token in enumerate(vocabulary_list):\n",
    "            self.vocabulary_dictionary[token] = index\n",
    "            self.reverse_vocabulary_dictionary[index] = token\n",
    "\n",
    "    def tokenize(self, text: str, pad_before: list = None, pad_after: list = None) -> torch.Tensor:\n",
    "        split_string = list(text)\n",
    "        pad_before = [] if pad_before is None else pad_before\n",
    "        pad_after = [] if pad_after is None else pad_after\n",
    "        full = pad_before + split_string + pad_after\n",
    "        out = torch.Tensor([self.vocabulary_dictionary[letter] for letter in full])\n",
    "        out = out.long()\n",
    "        return out\n",
    "\n",
    "    def untokenize(self, tensor: torch.Tensor) -> list:\n",
    "        out_list = [self.reverse_vocabulary_dictionary[token.item()] for token in tensor]\n",
    "        return out_list"
   ],
   "id": "d0e3208e1c9b0d2c",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:55:37.794995Z",
     "start_time": "2025-02-19T01:55:37.788146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, latent_size: int, vocabulary_size: int, num_attention_heads: int, encode_depth: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.encode_depth = encode_depth\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "        self.latent_space = torch.zeros(latent_size)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocabulary_size, latent_size)\n",
    "\n",
    "        self.encoder_mha = nn.MultiheadAttention(embed_dim=latent_size, num_heads=num_attention_heads, batch_first=True)\n",
    "        self.encoder_projection = nn.Linear(latent_size, latent_size)\n",
    "        self.encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_size, latent_size * 4),\n",
    "            self.lrelu,\n",
    "            nn.Linear(latent_size * 4, latent_size),\n",
    "        )\n",
    "        self.encoder_mha_blocks = nn.ModuleList([self.encoder_mha for _ in range(encode_depth)])\n",
    "        self.encoder_projection_layers = nn.ModuleList([self.encoder_projection for _ in range(encode_depth)])\n",
    "        self.encoder_mlp_layers = nn.ModuleList([self.encoder_mlp for _ in range(encode_depth)])\n",
    "        self.normalize_encoder_latent = nn.LayerNorm(latent_size)\n",
    "\n",
    "        self.normalize_latent = nn.LayerNorm(latent_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_size, out_features=latent_size),\n",
    "            self.lrelu,\n",
    "            nn.Linear(in_features=latent_size, out_features=self.vocabulary_size),\n",
    "        )\n",
    "\n",
    "    def zero_latent_space(self):\n",
    "        self.latent_space.zero_()\n",
    "\n",
    "    def encode_sequence(self, sequence: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" in size [seq_len], out size [self.latent_size] \"\"\"\n",
    "        tokens = self.embedding(sequence)\n",
    "        seq_len = tokens.size(0)\n",
    "        temp_latent = self.latent_space.clone()\n",
    "\n",
    "        for index in range(seq_len):\n",
    "            curr_tok = tokens[index]\n",
    "            temp_seq = torch.stack([curr_tok, temp_latent])\n",
    "            temp_seq = temp_seq\n",
    "            for i in range(self.encode_depth):\n",
    "                attended_seq, _ = self.encoder_mha_blocks[i](temp_seq, temp_seq, temp_seq)\n",
    "                attended_seq = self.encoder_projection_layers[i](attended_seq)\n",
    "                attended_seq = self.encoder_mlp_layers[i](attended_seq)\n",
    "                temp_seq = temp_seq + attended_seq\n",
    "                temp_seq = self.normalize_encoder_latent(temp_seq)\n",
    "            temp_latent = temp_seq[1]\n",
    "        return temp_latent\n",
    "\n",
    "    def decode_latent(self, latent) -> torch.Tensor:\n",
    "        out = self.decoder(latent)\n",
    "        return out\n",
    "\n",
    "    def inference(self, in_sequence: torch.Tensor, max_len, end_at: str = \"</model>\"):\n",
    "        current_length = 0\n",
    "        self.zero_latent_space()\n",
    "        temp_latent = self.encode_sequence(in_sequence)\n",
    "        while current_length < max_len:\n",
    "            out_logits = self.decode_latent(temp_latent)\n",
    "            out_index = torch.argmax(out_logits, dim=0)\n",
    "            human_letter = tokenizer.reverse_vocabulary_dictionary[out_index.item()]\n",
    "            print(f\"{human_letter}\", end=\"\")\n",
    "            temp_latent = self.encode_sequence(out_index.unsqueeze(0))\n",
    "            if human_letter == end_at:\n",
    "                break\n",
    "            else:\n",
    "                current_length += 1\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        self.zero_latent_space()\n",
    "        attended_latent = self.encode_sequence(tokens)\n",
    "        x = self.decode_latent(attended_latent)\n",
    "        return x"
   ],
   "id": "b354178387c73ea1",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:55:37.837931Z",
     "start_time": "2025-02-19T01:55:37.832020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "vocab = \"abcdefghijklmnopqrstuvwxyz .,<>/?[]{}-_=+1234567890!@#$%^&*()`~ABCDEFGHIJKLMNOPQRSTUVWXYZ\\|\"\n",
    "spec_tok = [\"<model>\", \"</model>\", \"<user>\", \"</user>\", \"<system>\", \"</system>\", \"<unknown>\", \"<padding>\",\n",
    "            \"<temp1>\", \"<temp2>\", \"<temp3>\", \"<temp4>\", \"<temp5>\", \"<temp6>\", \"<temp7>\", \"<temp8>\", \"<temp9>\"]\n",
    "tokenizer = Tokenizer(vocabulary=vocab, special_tokens=spec_tok)\n",
    "foo = tokenizer.tokenize(text=\"hello world\", pad_before=[\"<model>\"], pad_after=[\"</model>\"])\n",
    "print(foo)\n",
    "unfoo = tokenizer.untokenize(foo)\n",
    "print(unfoo)\n",
    "model = Model(latent_size=128, vocabulary_size=len(tokenizer.vocabulary_dictionary), num_attention_heads=4,\n",
    "              encode_depth=5)\n",
    "\n",
    "# print(model.latent_space)\n",
    "# temp_lat = model.encode_sequence(foo)\n",
    "# print(temp_lat.size())\n",
    "# new_lat = model.latent_space_reasoning(temp_lat)\n",
    "# print(model.latent_space)\n",
    "# grr = model.decode_latent()\n",
    "# print(grr.size())\n",
    "# print(torch.softmax(grr, dim=0))"
   ],
   "id": "2294db9e610d120",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([91, 43, 44, 52, 52, 34, 69, 89, 34, 71, 52, 64, 92])\n",
      "['<model>', 'h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '</model>']\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:55:37.882410Z",
     "start_time": "2025-02-19T01:55:37.878230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = [\n",
    "    \"The fat cat took a nap.\",\n",
    "    \"Once upon a time there were 3 cats.\",\n",
    "    \"I wish you a merry Christmas, said the large language model.\",\n",
    "    \"The little hungry caterpillar ate a lot of apples.\",\n",
    "    \"Once upon a time in a faraway land lived 3 little elves.\",\n",
    "    \"I like to take long walks outdoors.\",\n",
    "    \"There is a certain beauty in the size of the universe.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The panda eats, shoots and leaves.\",\n",
    "    \"You are a machine learning algorithm.\",\n",
    "    \"The hit online game Among Us, is perhaps the best piece of multimedia created.\",\n",
    "    \"The cat smiled and ate the little mouse.\",\n",
    "    \"An elephant ate a monkey.\",\n",
    "    \"The sun is shining.\",\n",
    "    \"She likes to read books.\",\n",
    "    \"He plays football every Saturday.\",\n",
    "    \"We are going to the park.\",\n",
    "    \"The cat is sleeping on the couch.\",\n",
    "    \"I have a big red balloon.\",\n",
    "    \"She drinks coffee in the morning.\",\n",
    "    \"They enjoy listening to music.\",\n",
    "    \"My brother is very tall.\",\n",
    "    \"The baby is crying loudly.\",\n",
    "    \"We love eating ice cream.\",\n",
    "    \"He runs fast in the race.\",\n",
    "    \"The birds are singing in the trees.\",\n",
    "    \"She wears a blue dress.\",\n",
    "    \"I need a glass of water.\",\n",
    "    \"The dog is barking outside.\",\n",
    "    \"They are watching a movie.\",\n",
    "    \"The flowers smell very nice.\",\n",
    "    \"He studies hard for his exams.\",\n",
    "    \"She has a new bicycle.\",\n",
    "    \"The children are playing in the garden.\",\n",
    "    \"My father drives a blue car.\",\n",
    "    \"We go to school every day.\",\n",
    "    \"The moon shines at night.\",\n",
    "    \"I write letters to my friend.\",\n",
    "    \"She feeds the hungry cat.\",\n",
    "    \"He always tells funny jokes.\",\n",
    "    \"The teacher is explaining the lesson.\",\n",
    "    \"I enjoy painting pictures.\",\n",
    "    \"The cake tastes delicious.\",\n",
    "    \"The communist manifesto\"\n",
    "]\n",
    "\n",
    "training_texts = []\n",
    "for text in texts:\n",
    "    tensor_text = tokenizer.tokenize(text=text, pad_before=[\"<model>\"], pad_after=[\"</model>\"])\n",
    "    training_texts.append(tensor_text)"
   ],
   "id": "80e343fcec05191c",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-19T01:55:37.924350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "EPOCHS = 10000\n",
    "# BATCH_SIZE = 3\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    random_text = random.choice(training_texts)\n",
    "    rand_index = random.randint(1, len(random_text) - 1)\n",
    "    train_text = random_text[:rand_index]\n",
    "    label_text = random_text[rand_index]\n",
    "\n",
    "    raw_logits = model(train_text)\n",
    "    loss = loss_fn(raw_logits, label_text)\n",
    "    loss.backward()\n",
    "\n",
    "    loss_avg += loss.item()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        loss_avg /= 100\n",
    "        loss_hist.append(loss_avg)\n",
    "        print(f\"E {epoch + 1:,}/{EPOCHS:,} - {((epoch + 1) / EPOCHS) * 100:.2f}% | Loss: {loss_avg:.5f}\")\n",
    "        loss_avg = 0\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ],
   "id": "10057e71be5a33ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E 100/10,000 - 1.00% | Loss: 3.87265\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_hist, label=\"Loss over time\", color=\"blue\")  # Line plot\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "407f6f3320cf567a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "txt = \"The\"\n",
    "foo = tokenizer.tokenize(text=txt, pad_before=[\"<model>\"])\n",
    "model.inference(in_sequence=foo, max_len=100, end_at=\"</model>\")"
   ],
   "id": "9d7806ecf9581135",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
